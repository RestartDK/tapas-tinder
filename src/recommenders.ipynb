{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.23.5\n",
    "%pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import ndcg_score\n",
    "from surprise import Reader, AlgoBase, Dataset, accuracy\n",
    "from surprise import SVD, KNNBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each CSV into a DataFrame\n",
    "df = pd.read_csv(\"data/clean_restaurant_data.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Train Split\n",
    "\n",
    "Implementing a **user stratified split** to ensure users in the test set also appear in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_stratified_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a user-stratified train-test split for a recommendation dataset.\n",
    "    This ensures users in the test set also appear in the training set.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with user_id and other columns\n",
    "        test_size: Proportion of data to include in the test set\n",
    "        random_state: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        train_df, test_df: DataFrames for training and testing\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Get users with their rating counts\n",
    "    user_counts = df[\"user_id\"].value_counts().reset_index()\n",
    "    user_counts.columns = [\"user_id\", \"rating_count\"]\n",
    "\n",
    "    # Separate users with multiple ratings and users with single rating\n",
    "    multi_rating_users = user_counts[user_counts[\"rating_count\"] > 1][\"user_id\"].values\n",
    "    single_rating_users = user_counts[user_counts[\"rating_count\"] == 1][\n",
    "        \"user_id\"\n",
    "    ].values\n",
    "\n",
    "    print(f\"Users with multiple ratings: {len(multi_rating_users)}\")\n",
    "    print(f\"Users with a single rating: {len(single_rating_users)}\")\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # For users with multiple ratings, split their ratings between train and test sets\n",
    "    for user in multi_rating_users:\n",
    "        user_indices = df[df[\"user_id\"] == user].index.tolist()\n",
    "\n",
    "        # Ensure at least one rating goes to train\n",
    "        n_test = max(1, int(len(user_indices) * test_size))\n",
    "        n_train = len(user_indices) - n_test\n",
    "\n",
    "        # If user has only two ratings, ensure one goes to each set\n",
    "        if len(user_indices) == 2:\n",
    "            n_test = 1\n",
    "            n_train = 1\n",
    "\n",
    "        # Randomly select indices for test set\n",
    "        user_test_indices = np.random.choice(user_indices, size=n_test, replace=False)\n",
    "        user_train_indices = list(set(user_indices) - set(user_test_indices))\n",
    "\n",
    "        train_indices.extend(user_train_indices)\n",
    "        test_indices.extend(user_test_indices)\n",
    "\n",
    "    # Calculate target test size and how many more samples we need\n",
    "    target_test_size = int(len(df) * test_size)\n",
    "    additional_test_samples_needed = max(0, target_test_size - len(test_indices))\n",
    "\n",
    "    # Determine how many single-rating users should go to test set\n",
    "    n_single_for_test = min(additional_test_samples_needed, len(single_rating_users))\n",
    "\n",
    "    if n_single_for_test > 0:\n",
    "        # Randomly select users with single ratings for test set\n",
    "        test_single_users = np.random.choice(\n",
    "            single_rating_users, size=n_single_for_test, replace=False\n",
    "        )\n",
    "\n",
    "        # Convert to a set for faster lookups\n",
    "        test_single_users_set = set(test_single_users)\n",
    "\n",
    "        # Add indices to train and test sets\n",
    "        for user in single_rating_users:\n",
    "            user_idx = df[df[\"user_id\"] == user].index[0]\n",
    "            if user in test_single_users_set:\n",
    "                test_indices.append(user_idx)\n",
    "            else:\n",
    "                train_indices.append(user_idx)\n",
    "    else:\n",
    "        # All single-rating users go to train set\n",
    "        for user in single_rating_users:\n",
    "            user_idx = df[df[\"user_id\"] == user].index[0]\n",
    "            train_indices.append(user_idx)\n",
    "\n",
    "    # Create the train and test dataframes\n",
    "    train_df = df.iloc[train_indices].copy().reset_index(drop=True)\n",
    "    test_df = df.iloc[test_indices].copy().reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train set size: {len(train_df)} ({len(train_df) / len(df):.2%})\")\n",
    "    print(f\"Test set size: {len(test_df)} ({len(test_df) / len(df):.2%})\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split\n",
    "# Check if 'rating_final' exists, if not, create it from 'rating'\n",
    "if \"rating_final\" not in df.columns and \"rating\" in df.columns:\n",
    "    print(\"Creating 'rating_final' column from 'rating' column\")\n",
    "    df[\"rating_final\"] = df[\"rating\"]\n",
    "\n",
    "# Create train-test split\n",
    "train_df, test_df = create_user_stratified_split(df, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Verify the split\n",
    "train_users = set(train_df[\"user_id\"].unique())\n",
    "test_users = set(test_df[\"user_id\"].unique())\n",
    "overlap_users = train_users.intersection(test_users)\n",
    "\n",
    "print(f\"\\nUsers in train set: {len(train_users)}\")\n",
    "print(f\"Users in test set: {len(test_users)}\")\n",
    "print(f\"Users in both train and test: {len(overlap_users)}\")\n",
    "print(\n",
    "    f\"Percentage of test users also in train: {len(overlap_users) / len(test_users):.2%}\"\n",
    ")\n",
    "\n",
    "# Check restaurant overlap\n",
    "train_restaurants = set(train_df[\"restaurant_id\"].unique())\n",
    "test_restaurants = set(test_df[\"restaurant_id\"].unique())\n",
    "overlap_restaurants = train_restaurants.intersection(test_restaurants)\n",
    "\n",
    "print(f\"\\nRestaurants in train set: {len(train_restaurants)}\")\n",
    "print(f\"Restaurants in test set: {len(test_restaurants)}\")\n",
    "print(f\"Restaurants in both train and test: {len(overlap_restaurants)}\")\n",
    "print(\n",
    "    f\"Percentage of test restaurants also in train: {len(overlap_restaurants) / len(test_restaurants):.2%}\"\n",
    ")\n",
    "\n",
    "# Make sure all required columns are present for Surprise\n",
    "required_columns = [\"user_id\", \"restaurant_id\", \"rating_final\"]\n",
    "for col in required_columns:\n",
    "    if col not in train_df.columns:\n",
    "        print(f\"ERROR: {col} missing from dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distributions between train and test sets\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=train_df, x=\"rating_final\", bins=5)\n",
    "plt.title(\"Rating Distribution - Train Set\")\n",
    "plt.xlabel(\"Rating\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=test_df, x=\"rating_final\", bins=5)\n",
    "plt.title(\"Rating Distribution - Test Set\")\n",
    "plt.xlabel(\"Rating\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise Dataset Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Create the Surprise Dataset from the train and test dataframes\n",
    "trainset = Dataset.load_from_df(\n",
    "    train_df[[\"user_id\", \"restaurant_id\", \"rating_final\"]], reader\n",
    ").build_full_trainset()\n",
    "\n",
    "testset = (\n",
    "    Dataset.load_from_df(test_df[[\"user_id\", \"restaurant_id\", \"rating_final\"]], reader)\n",
    "    .build_full_trainset()\n",
    "    .build_testset()\n",
    ")\n",
    "\n",
    "print(\"Conversion to Surprise Dataset complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommenders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomRecommender(AlgoBase):\n",
    "    \"\"\"\n",
    "    Random recommender based on the normal distribution of ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"\n",
    "        Predict a random rating from the normal distribution.\n",
    "        Note: u and i are not used as this is non-personalized.\n",
    "        \"\"\"\n",
    "        return np.random.normal(loc=self.train_mean, scale=self.train_std)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the trainset.\n",
    "        \"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # Calculate mean and standard deviation of all ratings\n",
    "        ratings = [r for (_, _, r) in self.trainset.all_ratings()]\n",
    "        self.train_mean = np.mean(ratings)\n",
    "        self.train_std = np.std(ratings)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "random_RS = MyRandomRecommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPopularRecommender(AlgoBase):\n",
    "    \"\"\"\n",
    "    Popularity-based recommender that predicts the average rating for each item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"\n",
    "        Predict the average rating for item i.\n",
    "        Note: u is not used as this is non-personalized.\n",
    "        \"\"\"\n",
    "        # Convert internal item id to raw item id\n",
    "        try:\n",
    "            raw_iid = self.trainset.to_raw_iid(i)\n",
    "        except ValueError:\n",
    "            # If item not in training set, return global mean\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        if raw_iid in self.mean_rating_per_item_df.index:\n",
    "            return self.mean_rating_per_item_df.loc[raw_iid][\"rating\"]\n",
    "        else:\n",
    "            # For items not in training set, return global mean\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the trainset.\n",
    "        \"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        # Convert internal ids to raw ids for better readability\n",
    "        ratings_list = [\n",
    "            (self.trainset.to_raw_iid(i), r)\n",
    "            for (_, i, r) in self.trainset.all_ratings()\n",
    "        ]\n",
    "\n",
    "        # Create DataFrame of all ratings\n",
    "        ratings_df = pd.DataFrame(ratings_list, columns=[\"item\", \"rating\"])\n",
    "\n",
    "        # Calculate mean rating per item\n",
    "        self.mean_rating_per_item_df = ratings_df.groupby(\"item\").agg(\n",
    "            {\"rating\": \"mean\"}\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "popular_RS = MyPopularRecommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_RS = SVD(\n",
    "    n_factors=100,  # Number of latent factors\n",
    "    n_epochs=20,  # Number of iterations over the training data\n",
    "    lr_all=0.005,  # Learning rate for all parameters\n",
    "    reg_all=0.02,  # Regularization term for all parameters\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-Based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_based_CF = KNNBaseline(\n",
    "    k=40,\n",
    "    min_k=1,\n",
    "    sim_options={\n",
    "        \"name\": \"cosine\",\n",
    "        \"user_based\": True,\n",
    "        \"min_support\": 3,  # Require at least 3 common items\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based_CF = KNNBaseline(\n",
    "    k=40,\n",
    "    min_k=1,\n",
    "    sim_options={\n",
    "        \"name\": \"cosine\",\n",
    "        \"user_based\": False,\n",
    "        \"min_support\": 3,  # Require at least 3 common users\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Aware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armed Bandits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation folds for later use\n",
    "def create_user_folds(df, n_splits=5):\n",
    "    \"\"\"\n",
    "    Create cross-validation folds stratified by user\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'user_id' and rating data\n",
    "        n_splits: Number of folds\n",
    "        random_state: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        List of (train_idx, test_idx) for each fold\n",
    "    \"\"\"\n",
    "    # Initialize group k-fold\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    # Use user as the group\n",
    "    groups = df[\"user_id\"].values\n",
    "\n",
    "    # Generate train/test indices for each fold\n",
    "    folds = list(group_kfold.split(df, groups=groups))\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "# Create CV folds\n",
    "cv_folds = create_user_folds(train_df, n_splits=5)\n",
    "\n",
    "# Count users and ratings in each fold\n",
    "fold_stats = []\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv_folds):\n",
    "    train_fold = train_df.iloc[train_idx]\n",
    "    test_fold = train_df.iloc[test_idx]\n",
    "\n",
    "    train_users = train_fold[\"user_id\"].nunique()\n",
    "    test_users = test_fold[\"user_id\"].nunique()\n",
    "    common_users = len(\n",
    "        set(train_fold[\"user_id\"]).intersection(set(test_fold[\"user_id\"]))\n",
    "    )\n",
    "\n",
    "    fold_stats.append(\n",
    "        {\n",
    "            \"Fold\": fold_idx + 1,\n",
    "            \"Train Ratings\": len(train_fold),\n",
    "            \"Test Ratings\": len(test_fold),\n",
    "            \"Train Users\": train_users,\n",
    "            \"Test Users\": test_users,\n",
    "            \"Common Users\": common_users,\n",
    "            \"User Coverage\": common_users / train_users * 100,\n",
    "        }\n",
    "    )\n",
    "\n",
    "fold_stats_df = pd.DataFrame(fold_stats)\n",
    "print(\"\\nCross-validation fold statistics:\")\n",
    "print(fold_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for each model and store them\n",
    "model_predictions = {}\n",
    "\n",
    "k_values = [5, 10, 20]\n",
    "\n",
    "models = {\n",
    "    \"Popular\": popular_RS,\n",
    "    \"Random\": random_RS,\n",
    "    \"SVD\": cf_RS,\n",
    "    \"User-Based CF\": user_based_CF,\n",
    "    \"Item-Based CF\": item_based_CF,\n",
    "}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(trainset)\n",
    "    model_predictions[model_name] = model.test(testset)\n",
    "\n",
    "print(\"Generated model predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "random_RS.fit(trainset)\n",
    "predictions_random = random_RS.test(testset)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"RMSE - Random Recommender: {accuracy.rmse(predictions_random, verbose=False):.4f}\"\n",
    ")\n",
    "print(f\"Training time: {random_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "popular_RS.fit(trainset)\n",
    "predictions_popular = popular_RS.test(testset)\n",
    "popular_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"RMSE - Popular Recommender: {accuracy.rmse(predictions_popular, verbose=False):.4f}\"\n",
    ")\n",
    "print(f\"Training time: {popular_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "cf_RS.fit(trainset)\n",
    "predictions_cf = cf_RS.test(testset)\n",
    "cf_time = time.time() - start_time\n",
    "\n",
    "print(f\"RMSE - CF SVD: {accuracy.rmse(predictions_cf, verbose=False):.4f}\")\n",
    "print(f\"Training time: {cf_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "user_based_CF.fit(trainset)\n",
    "predictions_user_based = user_based_CF.test(testset)\n",
    "user_based_time = time.time() - start_time\n",
    "\n",
    "print(f\"User-Based CF RMSE: {accuracy.rmse(predictions_user_based, verbose=False):.4f}\")\n",
    "print(f\"Training time: {user_based_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "item_based_CF.fit(trainset)\n",
    "predictions_item_based = item_based_CF.test(testset)\n",
    "item_based_time = time.time() - start_time\n",
    "\n",
    "print(f\"Item-Based CF RMSE: {accuracy.rmse(predictions_item_based, verbose=False):.4f}\")\n",
    "print(f\"Training time: {item_based_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics once for all models\n",
    "metrics = {}\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    metrics[model_name] = {\n",
    "        \"rmse\": accuracy.rmse(predictions, verbose=False),\n",
    "        \"mae\": accuracy.mae(predictions, verbose=False),\n",
    "    }\n",
    "\n",
    "# Print the metrics table\n",
    "print(\"\\n=== RMSE and MAE ===\")\n",
    "for model_name, model_metrics in metrics.items():\n",
    "    print(\n",
    "        f\"{model_name:<15} RMSE: {model_metrics['rmse']:.4f}, MAE: {model_metrics['mae']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "models = list(metrics.keys())\n",
    "rmse_values = [metrics[model][\"rmse\"] for model in models]\n",
    "mae_values = [metrics[model][\"mae\"] for model in models]\n",
    "\n",
    "# Set up the bar positions\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width / 2, rmse_values, width, label=\"RMSE\", color=\"skyblue\")\n",
    "plt.bar(x + width / 2, mae_values, width, label=\"MAE\", color=\"lightcoral\")\n",
    "\n",
    "# Customize the chart\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Model Performance Comparison: RMSE vs MAE\")\n",
    "plt.xticks(x, models, rotation=45, ha=\"right\")\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(rmse_values):\n",
    "    plt.text(i - width / 2, v, f\"{v:.4f}\", ha=\"center\", va=\"bottom\")\n",
    "for i, v in enumerate(mae_values):\n",
    "    plt.text(i + width / 2, v, f\"{v:.4f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings_df(predictions):\n",
    "    ratings = []\n",
    "    for pred in predictions:\n",
    "        ratings.append(\n",
    "            {\n",
    "                \"uid\": pred.uid,\n",
    "                \"iid\": pred.iid,\n",
    "                \"r_ui\": pred.r_ui,\n",
    "                \"est\": pred.est,\n",
    "                \"details\": pred.details,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(ratings)\n",
    "\n",
    "\n",
    "random_ratings_df = get_ratings_df(predictions_random)\n",
    "popular_ratings_df = get_ratings_df(predictions_popular)\n",
    "cf_ratings_df = get_ratings_df(predictions_cf)\n",
    "user_based_ratings_df = get_ratings_df(predictions_user_based)\n",
    "item_based_ratings_df = get_ratings_df(predictions_item_based)\n",
    "\n",
    "# Combine all ratings into a single dataframe for easier plotting\n",
    "all_cf_ratings = pd.concat(\n",
    "    [\n",
    "        random_ratings_df.assign(RS=\"Random\"),\n",
    "        popular_ratings_df.assign(RS=\"Popular\"),\n",
    "        cf_ratings_df.assign(RS=\"SVD CF\"),\n",
    "        user_based_ratings_df.assign(RS=\"User-Based CF\"),\n",
    "        item_based_ratings_df.assign(RS=\"Item-Based CF\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the distribution of estimated ratings\n",
    "plt.figure(figsize=(20, 9))\n",
    "sns.violinplot(x=\"RS\", y=\"est\", data=all_cf_ratings)\n",
    "plt.title(\"Estimated Ratings Distribution by Recommender System\")\n",
    "plt.xlabel(\"Recommender System\")\n",
    "plt.ylabel(\"Estimated Rating\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric functions\n",
    "def calculate_ndcg_at_k(predictions, k=10):\n",
    "    \"\"\"Calculate NDCG@k for a set of predictions.\"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, r_ui, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, r_ui))\n",
    "\n",
    "    ndcg_scores = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        if len(user_ratings) > k:\n",
    "            user_ratings = user_ratings[:k]\n",
    "\n",
    "        if len(user_ratings) > 1:\n",
    "            y_true = np.array([rating for _, rating in user_ratings]).reshape(1, -1)\n",
    "            y_score = np.array([est for est, _ in user_ratings]).reshape(1, -1)\n",
    "\n",
    "            try:\n",
    "                ndcg = ndcg_score(y_true, y_score)\n",
    "                ndcg_scores.append(ndcg)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user.\"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, r_ui, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, r_ui))\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        n_rel = sum((r_ui >= threshold) for (_, r_ui) in user_ratings)\n",
    "\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((est >= threshold) and (r_ui >= threshold))\n",
    "            for (est, r_ui) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        precision = (\n",
    "            n_rel_and_rec_k / min(k, len(user_ratings))\n",
    "            if min(k, len(user_ratings)) != 0\n",
    "            else 0\n",
    "        )\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "\n",
    "def mean_average_precision(predictions, threshold=3.5):\n",
    "    \"\"\"Calculate Mean Average Precision.\"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, r_ui, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, r_ui))\n",
    "\n",
    "    average_precisions = []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        relevant_count = 0\n",
    "        precisions = []\n",
    "\n",
    "        for i, (est, r_ui) in enumerate(user_ratings):\n",
    "            if r_ui >= threshold:\n",
    "                relevant_count += 1\n",
    "                precision_at_i = relevant_count / (i + 1)\n",
    "                precisions.append(precision_at_i)\n",
    "\n",
    "        ap = sum(precisions) / relevant_count if relevant_count > 0 else 0\n",
    "        average_precisions.append(ap)\n",
    "\n",
    "    return np.mean(average_precisions) if average_precisions else 0\n",
    "\n",
    "\n",
    "def calculate_coverage(predictions, catalog_size, threshold=3.5):\n",
    "    \"\"\"Calculate catalog coverage.\"\"\"\n",
    "    recommended_items = set()\n",
    "    for _, iid, _, est, _ in predictions:\n",
    "        if est >= threshold:\n",
    "            recommended_items.add(iid)\n",
    "\n",
    "    coverage = len(recommended_items) / catalog_size * 100  # As percentage\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complete Metrics Summary ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG@20</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>Precision@20</th>\n",
       "      <th>Recall@20</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Coverage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Popular</th>\n",
       "      <td>1.226855</td>\n",
       "      <td>0.855801</td>\n",
       "      <td>0.975245</td>\n",
       "      <td>0.973988</td>\n",
       "      <td>0.974044</td>\n",
       "      <td>0.685234</td>\n",
       "      <td>0.690304</td>\n",
       "      <td>0.683681</td>\n",
       "      <td>0.694612</td>\n",
       "      <td>0.683036</td>\n",
       "      <td>0.696385</td>\n",
       "      <td>0.792762</td>\n",
       "      <td>37.183766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>1.327087</td>\n",
       "      <td>1.024386</td>\n",
       "      <td>0.971646</td>\n",
       "      <td>0.970403</td>\n",
       "      <td>0.970434</td>\n",
       "      <td>0.576497</td>\n",
       "      <td>0.580493</td>\n",
       "      <td>0.574283</td>\n",
       "      <td>0.583523</td>\n",
       "      <td>0.573293</td>\n",
       "      <td>0.584602</td>\n",
       "      <td>0.790796</td>\n",
       "      <td>34.094090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.983307</td>\n",
       "      <td>0.760081</td>\n",
       "      <td>0.972685</td>\n",
       "      <td>0.971994</td>\n",
       "      <td>0.971719</td>\n",
       "      <td>0.781186</td>\n",
       "      <td>0.786347</td>\n",
       "      <td>0.780660</td>\n",
       "      <td>0.791843</td>\n",
       "      <td>0.780389</td>\n",
       "      <td>0.794006</td>\n",
       "      <td>0.791669</td>\n",
       "      <td>43.675315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User-Based CF</th>\n",
       "      <td>0.996891</td>\n",
       "      <td>0.767163</td>\n",
       "      <td>0.974067</td>\n",
       "      <td>0.973345</td>\n",
       "      <td>0.973403</td>\n",
       "      <td>0.782706</td>\n",
       "      <td>0.788061</td>\n",
       "      <td>0.782077</td>\n",
       "      <td>0.793513</td>\n",
       "      <td>0.781562</td>\n",
       "      <td>0.795445</td>\n",
       "      <td>0.792151</td>\n",
       "      <td>43.621488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item-Based CF</th>\n",
       "      <td>0.985807</td>\n",
       "      <td>0.761435</td>\n",
       "      <td>0.974921</td>\n",
       "      <td>0.974184</td>\n",
       "      <td>0.974068</td>\n",
       "      <td>0.784162</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.783513</td>\n",
       "      <td>0.795107</td>\n",
       "      <td>0.783370</td>\n",
       "      <td>0.797548</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>44.192055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   RMSE       MAE    NDCG@5   NDCG@10   NDCG@20  Precision@5  \\\n",
       "Popular        1.226855  0.855801  0.975245  0.973988  0.974044     0.685234   \n",
       "Random         1.327087  1.024386  0.971646  0.970403  0.970434     0.576497   \n",
       "SVD            0.983307  0.760081  0.972685  0.971994  0.971719     0.781186   \n",
       "User-Based CF  0.996891  0.767163  0.974067  0.973345  0.973403     0.782706   \n",
       "Item-Based CF  0.985807  0.761435  0.974921  0.974184  0.974068     0.784162   \n",
       "\n",
       "               Recall@5  Precision@10  Recall@10  Precision@20  Recall@20  \\\n",
       "Popular        0.690304      0.683681   0.694612      0.683036   0.696385   \n",
       "Random         0.580493      0.574283   0.583523      0.573293   0.584602   \n",
       "SVD            0.786347      0.780660   0.791843      0.780389   0.794006   \n",
       "User-Based CF  0.788061      0.782077   0.793513      0.781562   0.795445   \n",
       "Item-Based CF  0.789546      0.783513   0.795107      0.783370   0.797548   \n",
       "\n",
       "                    MAP  Coverage (%)  \n",
       "Popular        0.792762     37.183766  \n",
       "Random         0.790796     34.094090  \n",
       "SVD            0.791669     43.675315  \n",
       "User-Based CF  0.792151     43.621488  \n",
       "Item-Based CF  0.792424     44.192055  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define k values for evaluation\n",
    "k_values = [5, 10, 20]\n",
    "\n",
    "# Get total number of unique items for coverage calculation\n",
    "catalog_size = len(trainset.all_items())\n",
    "\n",
    "# Create empty metrics dictionary\n",
    "metrics = {model_name: {} for model_name in models}\n",
    "\n",
    "# Calculate all metrics for all models\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    # Error metrics\n",
    "    metrics[model_name][\"RMSE\"] = accuracy.rmse(predictions, verbose=False)\n",
    "    metrics[model_name][\"MAE\"] = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "    # NDCG@k\n",
    "    for k in k_values:\n",
    "        metrics[model_name][f\"NDCG@{k}\"] = calculate_ndcg_at_k(predictions, k=k)\n",
    "\n",
    "    # Precision and Recall@k\n",
    "    for k in k_values:\n",
    "        precision, recall = precision_recall_at_k(predictions, k=k)\n",
    "        metrics[model_name][f\"Precision@{k}\"] = precision\n",
    "        metrics[model_name][f\"Recall@{k}\"] = recall\n",
    "\n",
    "    # MAP\n",
    "    metrics[model_name][\"MAP\"] = mean_average_precision(predictions)\n",
    "\n",
    "    # Coverage\n",
    "    metrics[model_name][\"Coverage (%)\"] = calculate_coverage(predictions, catalog_size)\n",
    "\n",
    "# Convert metrics to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "\n",
    "# Display overview of all metrics\n",
    "print(\"\\n=== Complete Metrics Summary ===\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NDCG@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. NDCG@k Visualization\n",
    "print(\"\\n=== NDCG@k Metrics ===\")\n",
    "ndcg_cols = [col for col in metrics_df.columns if col.startswith(\"NDCG\")]\n",
    "display(metrics_df[ndcg_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap for NDCG scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    metrics_df[ndcg_cols],\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    cbar_kws={\"label\": \"NDCG Score\"},\n",
    ")\n",
    "plt.title(\"NDCG Scores by Model and k\")\n",
    "plt.ylabel(\"Recommender System\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision@k and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Precision and Recall Visualization\n",
    "print(\"\\n=== Precision and Recall Metrics ===\")\n",
    "precision_cols = [col for col in metrics_df.columns if col.startswith(\"Precision\")]\n",
    "recall_cols = [col for col in metrics_df.columns if col.startswith(\"Recall\")]\n",
    "display(metrics_df[precision_cols + recall_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision vs. Recall@10 scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, model in enumerate(metrics_df.index):\n",
    "    plt.scatter(\n",
    "        metrics_df.loc[model, \"Recall@10\"],\n",
    "        metrics_df.loc[model, \"Precision@10\"],\n",
    "        s=100,\n",
    "        label=model,\n",
    "    )\n",
    "    plt.annotate(\n",
    "        model,\n",
    "        (metrics_df.loc[model, \"Recall@10\"], metrics_df.loc[model, \"Precision@10\"]),\n",
    "        xytext=(10, 5),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Precision vs. Recall @10\", fontsize=16)\n",
    "plt.xlabel(\"Recall@10\")\n",
    "plt.ylabel(\"Precision@10\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Average Precision (MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Mean Average Precision Visualization\n",
    "print(\"\\n=== Mean Average Precision (MAP) ===\")\n",
    "display(metrics_df[[\"MAP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(metrics_df.index, metrics_df[\"MAP\"], color=\"purple\", alpha=0.7)\n",
    "plt.title(\"Mean Average Precision by Model\", fontsize=16)\n",
    "plt.ylabel(\"MAP Score (higher is better)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(metrics_df[\"MAP\"]):\n",
    "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage and Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Coverage Visualization\n",
    "print(\"\\n=== Coverage Metrics ===\")\n",
    "display(metrics_df[[\"Coverage (%)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort models by coverage\n",
    "coverage_sorted = metrics_df.sort_values(\"Coverage (%)\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(\n",
    "    coverage_sorted.index,\n",
    "    coverage_sorted[\"Coverage (%)\"],\n",
    "    color=sns.color_palette(\"viridis\", len(metrics_df)),\n",
    ")\n",
    "plt.title(\"Catalog Coverage by Model (%)\", fontsize=16)\n",
    "plt.xlabel(\"Coverage (%)\")\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(coverage_sorted[\"Coverage (%)\"]):\n",
    "    plt.text(v + 0.5, i, f\"{v:.1f}%\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
